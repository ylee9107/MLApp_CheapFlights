{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# App to find Cheap Flights\n",
    "\n",
    "# Introduction:\n",
    "\n",
    "In 2014, the cheapest fare from New York to Vienna was found to be around $800, but according to the advertised fares, where for a select no. of dates, these tickets were between $350 and $450. \n",
    "\n",
    "It all seemed to be a good deal and one might wonder if whether it is true or not. The industry does mistake the occasional mistakes on fares, because airlines occasionally and accidentally do happen to post fares that exclude fuel surcharges. Normally, it is expected that the advanced algorithms employed by these airlines would be updating fares that takes into account large number of factors, however due to the order generations of systems in place, mistakes do happen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Retrieving the Data from scraping the web:\n",
    "\n",
    "Fare data are obtained from a AJAX-based (Asynchornous JavaScript) webpage, this will require a browser to do the work. For such a task, there will be a need for two of the following pacakges: Selenium and ChromeDriver.\n",
    "\n",
    "- Selenium is a package for automating web browsers.\n",
    "- ChromeDriver is a headless browser, meaning there isn't a user interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "\n",
    "chromeDriver_file = 'chromedriver'\n",
    "chromeDriver_file_conda = 'chromedriver-binary alias'\n",
    "\n",
    "import os\n",
    "path = os.path.abspath(chromeDriver_file)\n",
    "print('pathway to ChromeDriver is: ' + '\\n' + path)\n",
    "\n",
    "# Set the ChromeDriver pathway:\n",
    "chromeDriver_path = path\n",
    "\n",
    "browser = webdriver.Chrome(chromeDriver_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Set the URL (from google flights):\n",
    "\n",
    "Dates are set to 1st of June to 15th of June in the year 2020 (note: that these dates can be changed to anything).\n",
    "\n",
    "NOTE: Need to use the Freebase IDs for city/region of interest for travel. for example, m/06y57 is for Sydney. m/0f04v is for empty search. m/02_286 is for NYC.\n",
    "\n",
    "It is possible to find it when searching in google from th ebelow link:\n",
    "https://www.google.com.mx/travel/guide?q=New+York+City&sa=X&rlz=1C1CHBD_esMX769MX769&output=search&tra=%5B%22AMAbHIJDZRALeKKuHEbLXHGOJ3aS9zzCTg:1579328461567%22,%22syndey%22,%22/m/02_286%22%5D&tcfs=EhUKCS9tLzAyXzI4NhIITmV3IFlvcms&dest_mid=/m/06y57#dest_mid=/m/06y57&tcfs=EiwKCC9tLzA2eTU3EgZTeWRuZXkaGAoKMjAyMC0wMi0wMxIKMjAyMC0wMi0wNw\n",
    "\n",
    "And to confirm it with the link below: make sure to control+f and search for Freebase ID.\n",
    "https://www.wikidata.org/wiki/Q3130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Input webpage as string:\n",
    "# flight_web_sats = 'https://www.google.com/travel/explore?tfs=CBsQAxojagcIARIDU1lEEgoyMDIwLTA2LTAxcgwIBBIIL20vMDJqOXoaI2oMCAQSCC9tLzAyajl6EgoyMDIwLTA2LTE1cgcIARIDU1lEcAFAAUgB&curr=AUD&gl=au&hl=en&authuser=0&origin=https%3A%2F%2Fwww.google.com&dest_mid'\n",
    "\n",
    "# # Retrieve the webpage's content using Selenium:\n",
    "# browser.get(flight_web_sats)\n",
    "\n",
    "# # Check the title of the webpage:\n",
    "# browser.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check to see if the required information from the webpage was captured: Take a Screenshot and save as 'test_flights.png'.\n",
    "# current_work_directory = os.getcwd()\n",
    "# sleep(30)\n",
    "# browser.save_screenshot(current_work_directory + '/test_flights.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Parsing the DOM to extract the individual flight data from the HTML tags:\n",
    "\n",
    "Document Object Model (DOM) is the collection of the individual elements on a webpage. These will include things like HTML tags, like 'body' and 'div', or classes and IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parsing:\n",
    "# soup = BeautifulSoup(browser.page_source, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract the individual city data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the city data:\n",
    "# # At the time of HTML scraping, the flight data was inside <div class='MeBuN'>\n",
    "# # Or the by XPATH: //*[@id=\"flt-app\"]/c-wiz/c-wiz/nav/div[1]/nav/div/div[2]/ol/li[1]/div\n",
    "\n",
    "# sleep(10)\n",
    "\n",
    "# flight_cards = soup.select('div[class*=MebuN]')\n",
    "\n",
    "# # Check out a single flight card:\n",
    "# flight_cards[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the piece of HTML information above, it can be noticed that the information needed is within the markup. \n",
    "For example: \n",
    "- the destination is seen here 'class=\"W6bZuc YMlIz\" London',\n",
    "- where the duration of the flight is 'class=\"Xq1DAb\" 1 day 3 hr 20 min' \n",
    "- and prices are located at 'class=\"QB2Jof xLPuCe\" datags=\"CidHQ2lQekJHLS0tLS0tLS0tcGZiMTI5QUFBQUFGNGlrY29Bby1aQUESATAaCwj+wAQQAhoDVVNE\">$739'.\n",
    "\n",
    "#### Next, is to obtain the required data from the markup:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # For-loop to extract the relevant information:\n",
    "# # At the time of HTML scraping, the price information are stored in <div class='MJg7fb'>\n",
    "\n",
    "# # for card in flight_cards:\n",
    "# #     print(card.select('h3')[0].text)\n",
    "# #     print(card.select('div[class*=MJg7fb]')[0].text)\n",
    "# #     print('\\n')\n",
    "\n",
    "# # Perform clean up of 'Great value' tags before the prices:\n",
    "# for card in flight_cards:\n",
    "#     print(card.select('h3')[0].text)\n",
    "#     print(card.select('div[class*=MJg7fb]')[0].text.replace('Great value',\"\"))\n",
    "#     print('\\n')\n",
    "\n",
    "# browser.quit()\n",
    "\n",
    "# print('Testing Complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, it can be confirmed that it is possible to retrieve the relevant data from the HTML. \n",
    "\n",
    "#### Next, is to retrieve flights that presents with the lowest cost and non-stop fares from the starting destination to the arrival destination. These flights would be for a 26 week period. All this is done by making a full scrape and parsing of a large number of fares.\n",
    "\n",
    "#### Import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from datetime import date, timedelta\n",
    "from time import sleep\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait \n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks dict:\n",
    "'''\n",
    "Func: check if the structure is empty or not.\n",
    "'''\n",
    "def is_empty(any_structure):\n",
    "    if any_structure:\n",
    "#         print('Structure is not empty.')\n",
    "        return False\n",
    "    else:\n",
    "#         print('Structure is empty.')\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#======= Restart ChromeDriver =========\n",
    "path = os.path.abspath(chromeDriver_file)\n",
    "print('pathway to ChromeDriver is: ' + '\\n' + path)\n",
    "print('\\n')\n",
    "\n",
    "# Set the ChromeDriver pathway:\n",
    "chromeDriver_path = path\n",
    "\n",
    "browser = webdriver.Chrome(chromeDriver_path)\n",
    "\n",
    "#======= Scraping the Web Data =========\n",
    "\n",
    "week_period = 26\n",
    "start_date = '2020-06-01'\n",
    "end_date = '2020-06-15'\n",
    "# start_date = '2020-04-01'\n",
    "# end_date = '2020-04-15'\n",
    "Adjust_delay = 50 # Adjust for internet connection.\n",
    "check_time = 7 # 7 days is teh default.\\\n",
    "\n",
    "check_wrong_startDate = datetime.datetime.strptime(start_date, '%Y-%m-%d')\n",
    "check_wrong_date = check_wrong_startDate - timedelta(days = check_time)\n",
    "check_wrong_date = str(check_wrong_date.date())\n",
    "\n",
    "departure_destination = \"Sydney\"\n",
    "arrival_destination = \"Europe\"\n",
    "\n",
    "\n",
    "\n",
    "# Format the flight dates: with the python datetime standard.\n",
    "startFlight_date = datetime.datetime.strptime(start_date, '%Y-%m-%d')\n",
    "endFlight_date = datetime.datetime.strptime(end_date, '%Y-%m-%d')\n",
    "\n",
    "# Dictionary for Fares:\n",
    "flightFare_dict = {}\n",
    "\n",
    "for idx in range(week_period):\n",
    "    sat_start = str(startFlight_date).split()[0]\n",
    "    sat_end = str(endFlight_date).split()[0]\n",
    "    flightFare_dict.update({sat_start: {}})\n",
    "    \n",
    "    # Load webpage:\n",
    "    sats = \"https://www.google.com/flights?hl=en#flt=..\" + sat_start + \"*..\" + sat_end + \";c:AUD;e:1;sd:1;t:h\"\n",
    "    sleep(np.random.randint(3,7))\n",
    "    browser.get(sats)\n",
    "    print('Index: ' + str(idx) + ' Starting Browser and searching link: Google ' + browser.title + '. Dates are: ' + sat_start + ' and ' + sat_end + '.' )\n",
    "    \n",
    "    # Input information to search for flights:\n",
    "    wait_10sec = WebDriverWait(browser, 10) # Seconds of Waiting.\n",
    "\n",
    "    print('Link Loaded, Entering Travel Details now.')\n",
    "\n",
    "    # Departure Search: input of departure location.\n",
    "    departureDestination_link = wait_10sec.until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"flt-app\"]/div[2]/main[1]/div[4]/div/div[3]/div/div[2]/div[1]')))\n",
    "    departureDestination_link.click()\n",
    "    departureDestination_link = wait_10sec.until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"sb_ifc50\"]/input')))\n",
    "    sleep(1)\n",
    "    departureDestination_link.send_keys(departure_destination)\n",
    "    sleep(2)\n",
    "    departureDestination_link.send_keys(Keys.ENTER)\n",
    "\n",
    "    # Arrival Search: input of arrival location.\n",
    "    arrivalDestination = wait_10sec.until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"flt-app\"]/div[2]/main[1]/div[4]/div/div[3]/div/div[2]/div[2]')))\n",
    "    arrivalDestination.click()\n",
    "    arrivalDestination = wait_10sec.until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"sb_ifc50\"]/input')))\n",
    "    sleep(1)\n",
    "    arrivalDestination.send_keys(arrival_destination)\n",
    "    sleep(2)\n",
    "    arrivalDestination.send_keys(Keys.ENTER)\n",
    "\n",
    "    # Get new URL:\n",
    "    sleep(1)\n",
    "    new_browser_url = browser.current_url\n",
    "    print('After inputting the destinations and searching, the new URL is: \\n' + new_browser_url)\n",
    "\n",
    "    # Finally, click on the 'Search' button:\n",
    "    floatingActionButton_click = browser.find_elements_by_xpath('//*[@id=\"flt-app\"]/div[2]/main[1]/div[4]/div/div[3]/div/div[4]/floating-action-button')[0]\n",
    "    sleep(2)\n",
    "    floatingActionButton_click.click()\n",
    "    print('Search done. Next is to get a list of the travel information.')\n",
    "    \n",
    "    \n",
    "    # Extract Relevant Data from webpage:\n",
    "    print('Collecting data.')\n",
    "    sleep(Adjust_delay)\n",
    "    soup = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "    flight_cards = soup.select('div[class*=MebuN]')\n",
    "    \n",
    "    previous_sat_start_date = 0\n",
    "    \n",
    "    for card in flight_cards:\n",
    "        print('Extracting...')\n",
    "        while True:\n",
    "            try:\n",
    "                city = card.select('h3')[0].text\n",
    "                fare = card.select('div[class*=MJg7fb]')[0].text.replace('Great value',\"\")    \n",
    "                print(city)\n",
    "                print(fare)\n",
    "                print('\\n')\n",
    "                flightFare_dict[sat_start] = {**flightFare_dict[sat_start], **{city: fare}}\n",
    "                   \n",
    "            except RuntimeError as detail:\n",
    "                print('Handling run-time error: ' + detail)\n",
    "                continue\n",
    "            break  \n",
    "\n",
    "\n",
    "    # Checks if the DICT in this current loop is empty, if YES, redo the data extraction:\n",
    "    previous_sat_start_date = startFlight_date - timedelta(days = check_time)\n",
    "    previous_sat_start_date = str(previous_sat_start_date.date())\n",
    "    \n",
    "    if ( previous_sat_start_date == sat_start ):\n",
    "        continue   \n",
    "    \n",
    "    elif ( previous_sat_start_date == check_wrong_date):\n",
    "        continue\n",
    "    \n",
    "    elif ( is_empty(flightFare_dict[previous_sat_start_date]) ):\n",
    "        print('This current index loop DICT was found to be empty, Retrying to collect Data.')\n",
    "        sleep(1)\n",
    "        for card in flight_cards:\n",
    "            print('Alternate try of extracting...')\n",
    "            city = card.select('h3')[0].text\n",
    "            fare = card.select('div[class*=MJg7fb]')[0].text.replace('Great value',\"\")    \n",
    "            print(city)\n",
    "            print(fare)\n",
    "            print('\\n')\n",
    "            flightFare_dict[sat_start] = {**flightFare_dict[sat_start], **{city: fare}}\n",
    "    else:\n",
    "        print('Error in collecting information, RETRY has FAILED.')\n",
    "        break\n",
    "    \n",
    "    \n",
    "    # Update the date: Add 7 days.\n",
    "    startFlight_date = startFlight_date + timedelta(days = check_time)\n",
    "    endFlight_date = endFlight_date + timedelta(days = check_time)\n",
    "    print('\\n')\n",
    "    \n",
    "browser.quit()\n",
    "print('Quiting Broswer, Data Collection Complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_empty(flightFare_dict[previous_sat_start_date]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# startFlight_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flightFare_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flight_cards = soup.select('div[class*=MebuN]')\n",
    "# flight_cards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Check out the data collected from the 26 week period:\n",
    "\n",
    "Begin with checking out flights heading to Berlin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking out outward flights to Berlin:\n",
    "\n",
    "city_key = 'Berlin'\n",
    "for key in flightFare_dict:\n",
    "    print(key, flightFare_dict[key][city_key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that there will be some need of further cleaning of the data, such as removing the 'A$'for AUD dollars from the price data and to remove the ',' commas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_dict = {}\n",
    "\n",
    "for k, v in flightFare_dict.items():\n",
    "    city_dict.update({k: int(v[city_key].replace(',','').split('$')[1])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some cleaning, it can be seen that the values appears as wanted. \n",
    "\n",
    "#### Plot the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert prices to integers:\n",
    "prices = [int(x) for x in city_dict.values()]\n",
    "\n",
    "# Extract the Date data from the dictionary:\n",
    "dates = city_dict.keys()\n",
    "\n",
    "# Plot:\n",
    "fig, ax = plt.subplots(figsize = (10, 6))\n",
    "plt.scatter(x = dates,\n",
    "            y = prices,\n",
    "            color = 'black',\n",
    "            s = 50           \n",
    "           )\n",
    "ax.set_xticklabels(dates, rotation = -70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot above, where they are flights from Sydney to Berlin, shows 26 consecutive weeks of data and have some variations to these fares. These can range from 900 AUD dollars to 1350 AUD dollars. It can also be said that initially from just eyeballing it, there isn't a certain pattern to these fares.\n",
    "\n",
    "#### Now, having a look at another City: Milan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking out outward flights to Milan:\n",
    "\n",
    "city_key = 'Milan'\n",
    "for key in flightFare_dict:\n",
    "    print(key, flightFare_dict[key][city_key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_dict = {}\n",
    "\n",
    "for k, v in flightFare_dict.items():\n",
    "    city_dict.update({k: int(v[city_key].replace(',','').split('$')[1])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert prices to integers:\n",
    "prices = [int(x) for x in city_dict.values()]\n",
    "\n",
    "# Extract the Date data from the dictionary:\n",
    "dates = city_dict.keys()\n",
    "\n",
    "# Plot:\n",
    "fig, ax = plt.subplots(figsize = (10, 6))\n",
    "plt.scatter(x = dates,\n",
    "            y = prices,\n",
    "            color = 'black',\n",
    "            s = 50           \n",
    "           )\n",
    "ax.set_xticklabels(dates, rotation = -70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot above, where they are flights from Sydney to Milan and the fares ranges from 1150 AUD dollars to 2250 AUD dollars. The types of cheap fares of interests can be seen from the dates of 26th Oct to 16th of Nov (around 1150 AUD dollars). These are the Outliers of interest.\n",
    "\n",
    "#### Next is to create an outlier detection system to notify the user of such cheap fares\n",
    "\n",
    "# 3 Outlier Detection System: for cheap flights:\n",
    "\n",
    "Outliers can be describe as an observation that lies outside the overall pattern of a distribution. (ref: http://mathworld.wolfram.com/Outlier.html)\n",
    "Such outliers can be determined by techniques that are both parametric and non-parametric. Once of the techniques utilised is the density-based spatial clustering of application with noise (DBSCAN). Another is called Grubb's Test. Depending on the type of data that present itself, where it can be multivariate or univariate, the technique utilised is different.\n",
    "\n",
    "For the purpose of this notebook, the data is a univariate time-series data, and that the technique to be used is called the Generalised Extreme Studentised Deviate (GESD) test specifically for outliers. (ref: http://www.real-statistics.com/students-t-distribution/identifying-outliers-using-t-distribution/generalized-extreme-studentized-deviate-test/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Import the required Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the data with probability plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, ax = plt.subplots(figsize = (10, 6))\n",
    "stats.probplot(list(city_dict.values()), plot = plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot above, . This assesses the normal probability (or the Q-Q plot, quantile-quantile). The diagonal straight line in the plot is the 'normal' line, and that any data close to this straight line is considered normal and not an outlier. The outliers are data points that veers off away from the straight line, or presents with a strong S-shape. \n",
    "\n",
    "In the case where there are more data available, it is more likely that the pattern/trend will be approximate to the diagonal straight line.\n",
    "\n",
    "## 3.2 Outlier detection code:\n",
    "\n",
    "#### Import the required library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyAstronomy import pyasl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fare_prices = prices\n",
    "max_outliers = 3\n",
    "significance_lvl = 0.025 #lower value means less sensitive to false positives in the algorithm\n",
    "\n",
    "r = pyasl.generalizedESD(fare_prices, max_outliers, significance_lvl, fullOutput = True)\n",
    "print('City -> ' + city_key)\n",
    "print('Total Outliers: ', r[0])\n",
    "\n",
    "# Print out data in regards to 'R' and Lambda: used to determine if data point is an outlier.\n",
    "out_dates = {}\n",
    "for i in sorted(r[1]):\n",
    "    out_dates.update({list(dates)[i]: list(prices)[i]})   \n",
    "print('Outlier Dates', out_dates.keys(), '\\n')\n",
    "print('        R         Lambda')\n",
    "\n",
    "# Make plot: outliers are in red colour.\n",
    "for i in range(len(r[2])):\n",
    "    print('%2d %8.5f %8.5f' % ((i+1), r[2][i], r[3][i]))\n",
    "    \n",
    "fig, ax = plt.subplots(figsize = (10, 6))\n",
    "plt.scatter(dates, prices, color = 'black', s=50)\n",
    "ax.set_xticklabels(dates, rotation =-70);\n",
    "\n",
    "for i in range(r[0]):\n",
    "    plt.plot(r[1][i], prices[r[1][i], 'rp'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
